{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#this is necessary for modernbert at the time of this writing, until future releases of transformers\n",
        "!pip install git+https://github.com/huggingface/transformers.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "yRg68-jTBY1h",
        "outputId": "5266d1b3-40a7-4ccc-fde6-9f512477af91"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/huggingface/transformers.git\n",
            "  Cloning https://github.com/huggingface/transformers.git to /tmp/pip-req-build-p0xeoq8w\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers.git /tmp/pip-req-build-p0xeoq8w\n",
            "  Resolved https://github.com/huggingface/transformers.git to commit 5fa35344755d8d9c29610b57d175efd03776ae9e\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.49.0.dev0) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.49.0.dev0) (0.27.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.49.0.dev0) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.49.0.dev0) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.49.0.dev0) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.49.0.dev0) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.49.0.dev0) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers==4.49.0.dev0) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.49.0.dev0) (0.5.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==4.49.0.dev0) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers==4.49.0.dev0) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers==4.49.0.dev0) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.49.0.dev0) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.49.0.dev0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.49.0.dev0) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.49.0.dev0) (2024.12.14)\n",
            "Building wheels for collected packages: transformers\n",
            "  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for transformers: filename=transformers-4.49.0.dev0-py3-none-any.whl size=10474239 sha256=4824dca9e49cb7cd5c07e05fdfb2e74b332b6298ef93b6229c1960f3e4191962\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-3m4ijcw8/wheels/32/4b/78/f195c684dd3a9ed21f3b39fe8f85b48df7918581b6437be143\n",
            "Successfully built transformers\n",
            "Installing collected packages: transformers\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.47.1\n",
            "    Uninstalling transformers-4.47.1:\n",
            "      Successfully uninstalled transformers-4.47.1\n",
            "Successfully installed transformers-4.49.0.dev0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "2GTpy_Fe1mw5",
        "outputId": "22a05c3a-18b7-4f8a-dd4e-f1685abebcb9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gradio\n",
            "  Downloading gradio-5.12.0-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu121)\n",
            "Collecting captum\n",
            "  Downloading captum-0.7.0-py3-none-any.whl.metadata (26 kB)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.13.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: shap in /usr/local/lib/python3.11/dist-packages (0.46.0)\n",
            "Collecting aiofiles<24.0,>=22.0 (from gradio)\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.7.1)\n",
            "Collecting fastapi<1.0,>=0.115.2 (from gradio)\n",
            "  Downloading fastapi-0.115.6-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting ffmpy (from gradio)\n",
            "  Downloading ffmpy-0.5.0-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting gradio-client==1.5.4 (from gradio)\n",
            "  Downloading gradio_client-1.5.4-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.25.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.27.1)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.1.5)\n",
            "Collecting markupsafe~=2.0 (from gradio)\n",
            "  Downloading MarkupSafe-2.1.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (1.26.4)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.10.14)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio) (24.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (11.1.0)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.10.5)\n",
            "Collecting pydub (from gradio)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting python-multipart>=0.0.18 (from gradio)\n",
            "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (6.0.2)\n",
            "Collecting ruff>=0.2.2 (from gradio)\n",
            "  Downloading ruff-0.9.2-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
            "Collecting safehttpx<0.2.0,>=0.1.6 (from gradio)\n",
            "  Downloading safehttpx-0.1.6-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting semantic-version~=2.0 (from gradio)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting starlette<1.0,>=0.40.0 (from gradio)\n",
            "  Downloading starlette-0.45.2-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting tomlkit<0.14.0,>=0.12.0 (from gradio)\n",
            "  Downloading tomlkit-0.13.2-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.15.1)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.12.2)\n",
            "Collecting uvicorn>=0.14.0 (from gradio)\n",
            "  Downloading uvicorn-0.34.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.5.4->gradio) (2024.10.0)\n",
            "Requirement already satisfied: websockets<15.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.5.4->gradio) (14.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.6.85)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from captum) (4.67.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.55.3)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from shap) (1.13.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from shap) (1.6.0)\n",
            "Requirement already satisfied: slicer==0.0.8 in /usr/local/lib/python3.11/dist-packages (from shap) (0.0.8)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.11/dist-packages (from shap) (0.60.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.11/dist-packages (from shap) (3.1.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Collecting starlette<1.0,>=0.40.0 (from gradio)\n",
            "  Downloading starlette-0.41.3-py3-none-any.whl.metadata (6.0 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (2024.12.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.25.1->gradio) (2.32.3)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0->gradio) (2.27.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba->shap) (0.43.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->shap) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->shap) (3.5.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.25.1->gradio) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.25.1->gradio) (2.3.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Downloading gradio-5.12.0-py3-none-any.whl (57.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio_client-1.5.4-py3-none-any.whl (321 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m321.4/321.4 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading captum-0.7.0-py3-none-any.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading fastapi-0.115.6-py3-none-any.whl (94 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.8/94.8 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading MarkupSafe-2.1.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (28 kB)\n",
            "Downloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
            "Downloading ruff-0.9.2-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading safehttpx-0.1.6-py3-none-any.whl (8.7 kB)\n",
            "Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Downloading starlette-0.41.3-py3-none-any.whl (73 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.2/73.2 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tomlkit-0.13.2-py3-none-any.whl (37 kB)\n",
            "Downloading uvicorn-0.34.0-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ffmpy-0.5.0-py3-none-any.whl (6.0 kB)\n",
            "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Installing collected packages: pydub, uvicorn, tomlkit, semantic-version, ruff, python-multipart, markupsafe, ffmpy, aiofiles, starlette, safehttpx, gradio-client, fastapi, gradio, captum\n",
            "  Attempting uninstall: markupsafe\n",
            "    Found existing installation: MarkupSafe 3.0.2\n",
            "    Uninstalling MarkupSafe-3.0.2:\n",
            "      Successfully uninstalled MarkupSafe-3.0.2\n",
            "Successfully installed aiofiles-23.2.1 captum-0.7.0 fastapi-0.115.6 ffmpy-0.5.0 gradio-5.12.0 gradio-client-1.5.4 markupsafe-2.1.5 pydub-0.25.1 python-multipart-0.0.20 ruff-0.9.2 safehttpx-0.1.6 semantic-version-2.10.0 starlette-0.41.3 tomlkit-0.13.2 uvicorn-0.34.0\n"
          ]
        }
      ],
      "source": [
        "!pip install gradio torch captum seaborn matplotlib shap"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import captum\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from captum.attr import LayerIntegratedGradients, TokenReferenceBase, visualization\n",
        "\n",
        "import gradio as gr\n",
        "import numpy as np\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "from captum.attr import (\n",
        "    LayerIntegratedGradients,\n",
        "    visualization as viz,\n",
        ")\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "from IPython.core.display import HTML\n",
        "import math"
      ],
      "metadata": {
        "id": "K6ZqpMaLa0c2"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = \"scbtm/ModernBERT_wine_quality_reviews_ft\"\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_path).to(device)\n",
        "model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "HoeLwLN7VkMT",
        "outputId": "f3f12d59-6ac2-4f55-cc9f-818fa166a586"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
            "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
            "You are not authenticated with the Hugging Face Hub in this notebook.\n",
            "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ModernBertForSequenceClassification(\n",
              "  (model): ModernBertModel(\n",
              "    (embeddings): ModernBertEmbeddings(\n",
              "      (tok_embeddings): Embedding(50368, 768, padding_idx=50283)\n",
              "      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (drop): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (layers): ModuleList(\n",
              "      (0): ModernBertEncoderLayer(\n",
              "        (attn_norm): Identity()\n",
              "        (attn): ModernBertAttention(\n",
              "          (Wqkv): Linear(in_features=768, out_features=2304, bias=False)\n",
              "          (rotary_emb): ModernBertRotaryEmbedding()\n",
              "          (Wo): Linear(in_features=768, out_features=768, bias=False)\n",
              "          (out_drop): Identity()\n",
              "        )\n",
              "        (mlp_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): ModernBertMLP(\n",
              "          (Wi): Linear(in_features=768, out_features=2304, bias=False)\n",
              "          (act): GELUActivation()\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "          (Wo): Linear(in_features=1152, out_features=768, bias=False)\n",
              "        )\n",
              "      )\n",
              "      (1-21): 21 x ModernBertEncoderLayer(\n",
              "        (attn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): ModernBertAttention(\n",
              "          (Wqkv): Linear(in_features=768, out_features=2304, bias=False)\n",
              "          (rotary_emb): ModernBertRotaryEmbedding()\n",
              "          (Wo): Linear(in_features=768, out_features=768, bias=False)\n",
              "          (out_drop): Identity()\n",
              "        )\n",
              "        (mlp_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): ModernBertMLP(\n",
              "          (Wi): Linear(in_features=768, out_features=2304, bias=False)\n",
              "          (act): GELUActivation()\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "          (Wo): Linear(in_features=1152, out_features=768, bias=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (final_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (head): ModernBertPredictionHead(\n",
              "    (dense): Linear(in_features=768, out_features=768, bias=False)\n",
              "    (act): GELUActivation()\n",
              "    (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (drop): Dropout(p=0.0, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=4, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_path)"
      ],
      "metadata": {
        "id": "Klv7xIk9X5HW"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def forward_with_ids(input_ids, attention_mask):\n",
        "    # pass them along to your model\n",
        "    outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "    return outputs.logits\n",
        "\n",
        "lig = LayerIntegratedGradients(forward_with_ids, model.model.embeddings)"
      ],
      "metadata": {
        "id": "7LhtqLjlFJDL"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def get_embeddings(model, tokenizer, device, text):\n",
        "\n",
        "    tokens = tokenizer(text, return_tensors=\"pt\", padding=True)\n",
        "    tokens = {k: v.to(device) for k, v in tokens.items()}\n",
        "\n",
        "    return model.model.embeddings(tokens['input_ids'])\n",
        "\n",
        "def generate_baseline(model, tokenizer, device, text):\n",
        "    tokens = tokenizer(text, return_tensors=\"pt\")\n",
        "    baseline_tokens = torch.ones_like(tokens['input_ids']) * tokenizer.pad_token_id\n",
        "    return model.model.embeddings(baseline_tokens.to(device))\n",
        "\n",
        "def get_target_class(model, tokenizer, device, text):\n",
        "    tokens = tokenizer(text, return_tensors=\"pt\", padding=True)\n",
        "    tokens = {k: v.to(device) for k, v in tokens.items()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**tokens)\n",
        "        pred_class = torch.argmax(outputs, dim=1)[0].item()\n",
        "        return pred_class\n",
        "\n",
        "\n",
        "def get_prediction_and_confidence(model, tokenizer, device, text):\n",
        "    tokens = tokenizer(text, return_tensors=\"pt\", padding=True)\n",
        "    tokens = {k: v.to(device) for k, v in tokens.items()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**tokens).logits\n",
        "        probs = F.softmax(outputs, dim=1)\n",
        "\n",
        "        if len(probs.shape) == 1:\n",
        "            probs = probs.unsqueeze(0)\n",
        "\n",
        "        pred_class = torch.argmax(probs, dim=1)[0].item()\n",
        "        #pred_class = min(pred_class, self.num_classes - 1)\n",
        "        confidence = probs[0, pred_class].item()\n",
        "\n",
        "    return pred_class, confidence, probs\n",
        "\n",
        "def get_explanations_ig(model, tokenizer, device, text):\n",
        "    # 1) Tokenize\n",
        "    tokens = tokenizer(text, return_tensors=\"pt\", padding=True)\n",
        "    tokens = {k: v.to(device) for k, v in tokens.items()}\n",
        "    input_ids = tokens[\"input_ids\"]\n",
        "    attention_mask = tokens[\"attention_mask\"]\n",
        "\n",
        "    # 2) Prepare baseline with same shape as input_ids\n",
        "    baseline_ids = torch.ones_like(input_ids) * tokenizer.pad_token_id\n",
        "    # If your model expects an attention mask for the baseline, you might do zeros:\n",
        "    baseline_mask = torch.zeros_like(attention_mask)\n",
        "    # or all 1s if you prefer. Depends on how you want to handle baseline.\n",
        "\n",
        "    # 3) Get predicted class\n",
        "    pred_class, confidence, probs = get_prediction_and_confidence(\n",
        "        model, tokenizer, device, text\n",
        "    )\n",
        "\n",
        "    # 4) Integrated Gradients\n",
        "    attributions_ig, delta_ig = lig.attribute(\n",
        "        inputs=(input_ids, attention_mask),\n",
        "        baselines=(baseline_ids, baseline_mask),\n",
        "        target=pred_class,\n",
        "        return_convergence_delta=True,\n",
        "        n_steps=50,\n",
        "    )\n",
        "\n",
        "    return attributions_ig, delta_ig, pred_class, confidence, probs\n",
        "\n",
        "def merge_subwords(tokens, attributions):\n",
        "    \"\"\"\n",
        "    Given:\n",
        "      tokens: list of subword tokens (e.g., [\"This\", \"Ġwine\", \"Ġis\", \"Ġfrom\", \"It\", \"aly\", \",\", \"ĠWhite\", \"ĠBl\", \"end\"])\n",
        "      attributions: np.array or list of float scores, same length as tokens\n",
        "\n",
        "    Returns:\n",
        "      merged_tokens, merged_attributions\n",
        "      where subwords have been combined back into words.\n",
        "      The attributions are summed by default, but you can choose to average, etc.\n",
        "    \"\"\"\n",
        "\n",
        "    merged_tokens = []\n",
        "    merged_attribs = []\n",
        "\n",
        "    current_word = \"\"\n",
        "    current_attr_sum = 0.0\n",
        "\n",
        "    for subword, attr in zip(tokens, attributions):\n",
        "        # Remove leading 'Ġ' if present\n",
        "        cleaned_sub = subword.lstrip(\"Ġ\")\n",
        "\n",
        "        # If the subword starts with \"Ġ\" or if current_word is empty,\n",
        "        # we treat it as the start of a new word.\n",
        "        # (For the very first token, current_word is empty, so we set it directly.)\n",
        "        if subword.startswith(\"Ġ\") or current_word == \"\":\n",
        "            # If we already have a word pending, push it first\n",
        "            if current_word != \"\":\n",
        "                merged_tokens.append(current_word)\n",
        "                merged_attribs.append(current_attr_sum)\n",
        "\n",
        "            current_word = cleaned_sub\n",
        "            current_attr_sum = attr\n",
        "        else:\n",
        "            # If subword doesn't start with 'Ġ', continue the same word\n",
        "            current_word += cleaned_sub  # e.g. \"Bl\" + \"end\" -> \"Blend\"\n",
        "            current_attr_sum += attr\n",
        "\n",
        "    # After the loop, push the final word\n",
        "    if current_word:\n",
        "        merged_tokens.append(current_word)\n",
        "        merged_attribs.append(current_attr_sum)\n",
        "\n",
        "    return merged_tokens, merged_attribs\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "def interpret_sentence(model, tokenizer, device, text):\n",
        "    attributions_ig, delta_ig, pred_class, confidence, probs = get_explanations_ig(model, tokenizer, device, text)\n",
        "\n",
        "    id2label = {0: 'bad', 1: 'average', 2: 'good', 3: 'excellent'}\n",
        "    label_str = id2label[pred_class]\n",
        "\n",
        "    attributions = attributions_ig.sum(dim=2).squeeze(0)\n",
        "    attributions = attributions / torch.norm(attributions)\n",
        "    attributions = attributions.cpu().detach().numpy()\n",
        "\n",
        "    #tokens_list = tokenizer.tokenize(text)\n",
        "    #clean_tokens_list = [token.replace(\"Ġ\", \"\") for token in tokens_list]\n",
        "\n",
        "    tokens_list = tokenizer.tokenize(text)\n",
        "    # Suppose `attributions` is a 1D numpy array (or list) matching `tokens_list`\n",
        "    merged_tokens, merged_attributions = merge_subwords(tokens_list, attributions)\n",
        "\n",
        "\n",
        "\n",
        "    assert len(tokens_list) <= len(attributions), \"Tokens must not be longer than attributions\"\n",
        "\n",
        "    visualization = viz.VisualizationDataRecord(word_attributions = merged_attributions, #attributions,\n",
        "                                                pred_prob = confidence, #probs,\n",
        "                                                pred_class = pred_class,\n",
        "                                                true_class = None,\n",
        "                                                attr_class = None,\n",
        "                                                attr_score = sum(merged_attributions), #attributions.sum(),\n",
        "                                                raw_input_ids = merged_tokens, #clean_tokens_list, #text,\n",
        "                                                convergence_score = delta_ig)\n",
        "\n",
        "    # Step B: Convert Captum HTML to a string\n",
        "    # visualize_text expects an iterable of data records, so pass [record]\n",
        "    out_html = viz.visualize_text([visualization])\n",
        "    # out_html is an IPython.core.display.HTML object\n",
        "    raw_html_str = out_html.data  # this is the underlying HTML string\n",
        "\n",
        "    # Step C: Return the label, confidence, and the HTML string\n",
        "    return label_str, confidence, raw_html_str\n",
        "\"\"\"\n",
        "\n",
        "#--------------\n",
        "\n",
        "def format_word_importances_only(words, importances):\n",
        "    \"\"\"\n",
        "    Returns a single <td> HTML cell containing the tokens\n",
        "    colored according to their attributions.\n",
        "    \"\"\"\n",
        "    if importances is None or len(importances) == 0:\n",
        "        return \"<td></td>\"\n",
        "    assert len(words) <= len(importances), (\n",
        "        f\"Found more tokens than attributions. \"\n",
        "        f\"len(words)={len(words)} len(importances)={len(importances)}\"\n",
        "    )\n",
        "\n",
        "    # Build HTML for token coloring\n",
        "    token_html_list = []\n",
        "    for word, imp in zip(words, importances[: len(words)]):\n",
        "        color = _get_color(imp)\n",
        "        token_html_list.append(\n",
        "            f\"<mark style='background-color:{color}; \"\n",
        "            \"display:inline-block; line-height:1.75; \"\n",
        "            \"margin:0 2px; border-radius:3px'>\"\n",
        "            f\"{word}</mark>\"\n",
        "        )\n",
        "    return \"<td>\" + \"\".join(token_html_list) + \"</td>\"\n",
        "\n",
        "def _get_color(attr):\n",
        "    \"\"\"\n",
        "    Internal helper to map attribution value to an RGBA color.\n",
        "    This replicates Captum's logic for coloring tokens.\n",
        "    \"\"\"\n",
        "    # You can tweak these constants if desired\n",
        "    # Positive attributions = green, negative = red, magnitude scales opacity\n",
        "    # If you don't have negative attributions, you can just do greens, etc.\n",
        "    red, green, blue = (21, 177, 234) if attr > 0 else (234, 78, 21) #(255, 0, 0) if attr < 0 else (0, 255, 0)\n",
        "    alpha = min(1.0, 0.2 + math.fabs(attr))  # scale transparency by magnitude\n",
        "    return f\"rgba({red},{green},{blue},{alpha})\"\n",
        "\n",
        "def visualize_text_only_word_importances(datarecords):\n",
        "    \"\"\"\n",
        "    datarecords can be either a single VisualizationDataRecord or a list of them.\n",
        "    Generates an HTML table with ONLY the 'Word Importance' column, removing\n",
        "    'True Label', 'Predicted Label', 'Attribution Label', and 'Attribution Score'.\n",
        "    \"\"\"\n",
        "\n",
        "    # If a single VisualizationDataRecord is passed, wrap in a list\n",
        "    if not isinstance(datarecords, list):\n",
        "        datarecords = [datarecords]\n",
        "\n",
        "    # Table header: only one column (Word Importance)\n",
        "    rows = [\"<th>Word Importance</th>\"]\n",
        "\n",
        "    # For each VisualizationDataRecord, format one <tr>\n",
        "    for dr in datarecords:\n",
        "        # dr.word_attributions: np.array or list of attributions\n",
        "        # dr.raw_input_ids: tokens\n",
        "        word_importances_html = format_word_importances_only(\n",
        "            dr.raw_input_ids, dr.word_attributions\n",
        "        )\n",
        "        row_html = f\"<tr>{word_importances_html}</tr>\"\n",
        "        rows.append(row_html)\n",
        "\n",
        "    table_html = \"<table>\" + \"\".join(rows) + \"</table>\"\n",
        "    return HTML(table_html)\n",
        "#----------------------------\n",
        "\n",
        "def interpret_sentence(model, tokenizer, device, text):\n",
        "    attributions_ig, delta_ig, pred_class, confidence, probs = get_explanations_ig(model, tokenizer, device, text)\n",
        "\n",
        "    id2label = {0: 'bad', 1: 'average', 2: 'good', 3: 'excellent'}\n",
        "    label_str = id2label[pred_class]\n",
        "\n",
        "    attributions = attributions_ig.sum(dim=2).squeeze(0)\n",
        "    attributions = attributions / torch.norm(attributions)\n",
        "    attributions = attributions.cpu().detach().numpy()\n",
        "\n",
        "    #tokens_list = tokenizer.tokenize(text)\n",
        "    #clean_tokens_list = [token.replace(\"Ġ\", \"\") for token in tokens_list]\n",
        "\n",
        "    tokens_list = tokenizer.tokenize(text)\n",
        "    # Suppose `attributions` is a 1D numpy array (or list) matching `tokens_list`\n",
        "    merged_tokens, merged_attributions = merge_subwords(tokens_list, attributions)\n",
        "\n",
        "\n",
        "\n",
        "    assert len(tokens_list) <= len(attributions), \"Tokens must not be longer than attributions\"\n",
        "\n",
        "    visualization = viz.VisualizationDataRecord(word_attributions = merged_attributions, #attributions,\n",
        "                                                pred_prob = confidence, #probs,\n",
        "                                                pred_class = pred_class,\n",
        "                                                true_class = None,\n",
        "                                                attr_class = None,\n",
        "                                                attr_score = np.mean(merged_attributions), #sum(merged_attributions), #attributions.sum(),\n",
        "                                                raw_input_ids = merged_tokens, #clean_tokens_list, #text,\n",
        "                                                convergence_score = delta_ig)\n",
        "\n",
        "    # Step B: Convert Captum HTML to a string\n",
        "    # visualize_text expects an iterable of data records, so pass [record]\n",
        "    #out_html = viz.visualize_text([visualization])\n",
        "    # out_html is an IPython.core.display.HTML object\n",
        "    #raw_html_str = out_html.data  # this is the underlying HTML string\n",
        "    html_obj = visualize_text_only_word_importances(visualization)\n",
        "    # This is an IPython HTML object; get the raw string via `.data`\n",
        "    raw_html_str = html_obj.data\n",
        "\n",
        "    # Step C: Return the label, a percentage format confidence, and the HTML string\n",
        "    return label_str, f'{100*confidence:.1f} %', raw_html_str\n",
        "\n",
        "def create_gradio_interface():\n",
        "    from functools import partial\n",
        "    final_fn = partial(interpret_sentence, model, tokenizer, device)\n",
        "    iface = gr.Interface(\n",
        "        fn=final_fn,\n",
        "        inputs=gr.Textbox(lines=5, label=\"Input Text\"),\n",
        "        outputs=[\n",
        "            gr.Label(label=\"Predicted Class\"),\n",
        "            gr.Label(label=\"Confidence\"),\n",
        "            gr.HTML(label=\"Attribution Visualization\"), #gr.Plot(label=\"Attribution Analysis\")\n",
        "        ],\n",
        "        title=\"Advanced XAI Text Classification Explainer\",\n",
        "        description=\"\"\"This application provides sophisticated explanations for text classification predictions using multiple XAI methods:\n",
        "        1. Integrated Gradients: Shows how predictions change from baseline to input\"\"\",\n",
        "        examples=[\n",
        "            [\"This wine is from Italy, White Blend variety. Aromas include tropical fruit, broom, brimstone and dried herb. The palate isn't overly expressive, offering unripened apple, citrus and dried sage alongside brisk acidity.\"],\n",
        "            [\"This wine is from US, Pinot Noir variety. Much like the regular bottling from 2012, this comes across as rather rough and tannic, with rustic, earthy, herbal characteristics. Nonetheless, if you think of it as a pleasantly unfussy country wine, it's a good companion to a hearty winter stew.\"],\n",
        "            [\"Baked plum, molasses, balsamic vinegar and cheesy oak aromas feed into a palate that's braced by a bolt of acidity. A compact set of saucy red-berry and plum flavors features tobacco and peppery accents, while the finish is mildly green in flavor, with respectable weight and balance.\"]\n",
        "        ]\n",
        "    )\n",
        "    return iface"
      ],
      "metadata": {
        "id": "Y7a7jIvhWybf"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "iface = create_gradio_interface()\n",
        "iface.launch(debug=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 677
        },
        "id": "Khud_Dw1iczV",
        "outputId": "cae089f2-01b9-4cc8-dc0c-a74ac5277e49"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://b1e7277358b56d4f90.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://b1e7277358b56d4f90.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://b1e7277358b56d4f90.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sudGo8hFTzEb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CRDpYj2TTy6o"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}